# Shortcut generator config for BVH + ARKit format
# Body: 225 dims (75 joints Ã— 3 axis-angle)
# Face: 51 dims (ARKit blendshapes)
#
# Train with: python train.py --config configs/shortcut_bvh_arkit.yaml

is_train: True
ddp: False
stat: ts
root_path: ./
out_path: ./outputs/shortcut_bvh_arkit/
output_dir: ./outputs/
project: shortcut_bvh_arkit

# BVH FORMAT FLAG - tells trainer to use BVH mode
use_bvh_format: True

# WandB config
wandb_project: gesturelcm_bvh
wandb_entity: null
wandb_log_dir: ./outputs/wandb/

# Skip SMPL-X evaluation in BVH mode (no eval model needed)
skip_eval_model: True

# Data paths - BVH + ARKit format
data_path: ./datasets/BEAT/beat_english_v0.2.1/beat_english_v0.2.1/
data_path_1: ./datasets/hub/
pose_norm: True

# Data loader config for BVH
data:
  name_pyfile: dataloaders.beat_normalized
  class_name: BEATNormalizedDataset
  data_path: ./datasets/BEAT/beat_english_v0.2.1/beat_english_v0.2.1/
  cache_path: ./datasets/beat_cache/beat_bvh_arkit/
  pose_fps: 30
  audio_sr: 16000
  pose_length: 128
  stride: 20
  training_speakers: [2]
  new_cache: False
  train_bs: 128
  test_clip: False
  onset_rep: False

# Model config (inlined from sc_model_bvh_config.yaml)
model:
  model_name: LSM
  g_name: GestureLSM
  do_classifier_free_guidance: False
  guidance_scale: 2
  n_steps: 4
  use_exp: False

  denoiser:
    target: models.denoiser.GestureDenoiser
    params:
      input_dim: 256  # body (128) + face (128) latent codes
      latent_dim: 256
      ff_size: 1024
      num_layers: 8
      num_heads: 4
      dropout: 0.1
      activation: "gelu"
      n_seed: 8
      flip_sin_to_cos: True
      freq_shift: 0.0
      cond_proj_dim: 256
      use_exp: False

  modality_encoder:
    target: models.layers.modality_encoder.ModalityEncoder
    params:
      data_path: ./
      t_fix_pre: False
      audio_dim: 128
      audio_in: 128
      raw_audio: False
      latent_dim: 256
      audio_fps: 30
      use_exp: False

# Normalization stats (generated by train_vqvae_bvh.py)
mean_pose_path: ./outputs/vqvae_bvh/body/body_mean.npy
std_pose_path: ./outputs/vqvae_bvh/body/body_std.npy
mean_face_path: ./outputs/vqvae_bvh/face/face_mean.npy
std_face_path: ./outputs/vqvae_bvh/face/face_std.npy

# VQ-VAE paths (train these first with train_vqvae_bvh.py!)
vqvae_body_path: ./outputs/vqvae_bvh/body/best.pth
vqvae_face_path: ./outputs/vqvae_bvh/face/best.pth

# We use single body VQ-VAE instead of separate upper/hands/lower
use_separate_body_parts: False
use_trans: False

decay_epoch: 500

vqvae_squeeze_scale: 4
vqvae_latent_scale: 5

vae_test_len: 32
vae_test_dim: 225
vae_test_stride: 20
vae_length: 240
vae_codebook_size: 1024
vae_layer: 4
vae_grow: [1,1,2,1]
variational: False

# Data config
training_speakers: [2]
additional_data: False
cache_path: datasets/beat_cache/beat_bvh_arkit/
dataset: beat_bvh_arkit
new_cache: False

# Motion config - BVH with axis-angle
ori_joints: beat_bvh_joints
tar_joints: beat_bvh_full
pose_rep: bvh_axis_angle
pose_fps: 30
rot6d: False
pre_frames: 4
pose_dims: 225
pose_length: 128
stride: 20
test_length: 128
m_fix_pre: False

# Audio config
audio_rep: mel
audio_sr: 16000
audio_fps: 16000
audio_norm: False
audio_f: 128
audio_raw: None

# Text config (optional - can disable if not using)
word_rep: none
word_dims: 0
t_pre_encoder: none

# Face config - ARKit 51 blendshapes
facial_rep: arkit_51
facial_dims: 51
facial_norm: True
facial_f: 0

# Speaker config
id_rep: onehot
speaker_f: 0

# Training config
batch_size: 128
lr_base: 2e-4
trainer: shortcut_bvh

rec_weight: 1
grad_norm: 0.99
epochs: 500
test_period: 20
val_period: 20
log_period: 50
debug: False

# Solver config (required by trainer)
solver:
  epochs: 500
  grad_norm: 0.99
  optimizer: AdamW
  lr: 2e-4
  weight_decay: 0.0001
  betas: [0.9, 0.999]

# GPU
gpus: [0]
seed: 42
